{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Reinforcement Learning for Atari with Improvised Architecture for Breakout-v0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from __future__ import division\n",
    "from PIL import Image\n",
    "\n",
    "# From openai.gym (pip install gym)\n",
    "import gym\n",
    "\n",
    "# From Keras-RL (pip install keras-rl)\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Flatten, Dropout, concatenate, Permute\n",
    "from keras.layers import Input, Conv2D\n",
    "from keras.activations import relu, linear\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import LinearAnnealedPolicy, BoltzmannQPolicy\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.core import Processor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Defining class for pre-processing the game_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GameProcess(Processor):\n",
    "    def process_observation(self, observation):\n",
    "        img = Image.fromarray(observation)\n",
    "        img = np.array(img.resize(frame_shape).convert('L'))\n",
    "        return img.astype('uint8')  \n",
    "\n",
    "    def process_state_batch(self, batch):\n",
    "        batch = batch.astype('float32') / 255.\n",
    "        return batch\n",
    "\n",
    "    def process_reward(self, reward):\n",
    "        return np.clip(reward, -1., 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breakout Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "\n",
    "# random seed to reproduce game states\n",
    "np.random.seed(125)\n",
    "env.seed(125)\n",
    "\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Number of possible action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_actions = env.action_space.n\n",
    "nb_actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Taking stack of 4 consecutive frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 84, 84)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frame_shape = (84, 84)\n",
    "window_length = 4\n",
    "input_shape = (window_length,) + frame_shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define fire_incept modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fire_incept(x, squeeze=16, expand=64):\n",
    "    x = Conv2D(squeeze, (3,3))(x)\n",
    "    x = Activation('relu')(x)\n",
    "    \n",
    "    left = Conv2D(expand, (3,3), padding='same')(x)\n",
    "    left = Activation('relu')(left)\n",
    "    \n",
    "    right = Conv2D(expand, (5,5), padding='same')(x)\n",
    "    right = Activation('relu')(right)\n",
    "    \n",
    "    x = concatenate([left, right], axis=3)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 4, 84, 84)     0                                            \n",
      "____________________________________________________________________________________________________\n",
      "permute_1 (Permute)              (None, 84, 84, 4)     0           input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)                (None, 26, 26, 64)    12608       permute_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)                (None, 11, 11, 64)    102464      conv2d_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)                (None, 9, 9, 16)      9232        conv2d_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_1 (Activation)        (None, 9, 9, 16)      0           conv2d_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)                (None, 9, 9, 64)      9280        activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)                (None, 9, 9, 64)      25664       activation_1[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_2 (Activation)        (None, 9, 9, 64)      0           conv2d_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_3 (Activation)        (None, 9, 9, 64)      0           conv2d_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)      (None, 9, 9, 128)     0           activation_2[0][0]               \n",
      "                                                                   activation_3[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)                (None, 7, 7, 32)      36896       concatenate_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_4 (Activation)        (None, 7, 7, 32)      0           conv2d_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)                (None, 7, 7, 64)      18496       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)                (None, 7, 7, 64)      51264       activation_4[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "activation_5 (Activation)        (None, 7, 7, 64)      0           conv2d_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_6 (Activation)        (None, 7, 7, 64)      0           conv2d_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)      (None, 7, 7, 128)     0           activation_5[0][0]               \n",
      "                                                                   activation_6[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)              (None, 7, 7, 128)     0           concatenate_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)                (None, 5, 5, 64)      73792       dropout_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 1600)          0           conv2d_9[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 512)           819712      flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 512)           0           dense_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 4)             2052        dropout_2[0][0]                  \n",
      "====================================================================================================\n",
      "Total params: 1,161,460\n",
      "Trainable params: 1,161,460\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "game_input=Input(shape=input_shape)\n",
    "\n",
    "x = Permute((2, 3, 1))(game_input)\n",
    "\n",
    "x = Conv2D(64, (7,7), strides=(3, 3), activation='relu')(x)\n",
    "\n",
    "x = Conv2D(64, (5,5), strides=(2, 2), activation='relu')(x)\n",
    "\n",
    "x = fire_incept(x, squeeze=16, expand=64)\n",
    "\n",
    "x = fire_incept(x, squeeze=32, expand=64)\n",
    "\n",
    "x = Dropout(0.4)(x)\n",
    "\n",
    "x = Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "x = Dropout(0.3)(x)\n",
    "\n",
    "out = Dense(nb_actions, activation='linear')(x)\n",
    "\n",
    "model_fire_incept= Model(game_input, out)\n",
    "model_fire_incept.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuring the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Allocating memory for experience replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "memory = SequentialMemory(limit=1000000, window_length=window_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Policy: Boltzmann Exploration\n",
    ": Gradually exploration will be decreased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "policy = LinearAnnealedPolicy(BoltzmannQPolicy(), attr='tau', value_max=1., value_min=.2, value_test=.1, nb_steps=1000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Compiling DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn = DQNAgent(model=model_fire_incept, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               nb_steps_warmup=50000, gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.compile(Adam(lr=.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Check if Agent is learning for first 0.5M Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 500000 steps ...\n",
      "Interval 1 (0 steps performed)\n",
      "10000/10000 [==============================] - 67s - reward: 0.0067    - ETA: 0s - rewa\n",
      "54 episodes - episode_reward: 1.241 [0.000, 5.000] - ale.lives: 2.906\n",
      "\n",
      "Interval 2 (10000 steps performed)\n",
      "10000/10000 [==============================] - 66s - reward: 0.0069    - ETA: 0s - rewa\n",
      "55 episodes - episode_reward: 1.236 [0.000, 6.000] - ale.lives: 2.941\n",
      "\n",
      "Interval 3 (20000 steps performed)\n",
      "10000/10000 [==============================] - 65s - reward: 0.0073    \n",
      "54 episodes - episode_reward: 1.370 [0.000, 6.000] - ale.lives: 2.841\n",
      "\n",
      "Interval 4 (30000 steps performed)\n",
      "10000/10000 [==============================] - 64s - reward: 0.0061    \n",
      "56 episodes - episode_reward: 1.089 [0.000, 6.000] - ale.lives: 2.902\n",
      "\n",
      "Interval 5 (40000 steps performed)\n",
      "10000/10000 [==============================] - 65s - reward: 0.0065    \n",
      "56 episodes - episode_reward: 1.161 [0.000, 5.000] - ale.lives: 2.934\n",
      "\n",
      "Interval 6 (50000 steps performed)\n",
      "10000/10000 [==============================] - 287s - reward: 0.0057   \n",
      "57 episodes - episode_reward: 1.000 [0.000, 3.000] - loss: 0.003 - mean_absolute_error: 0.010 - mean_q: 0.013 - mean_tau: 0.956 - ale.lives: 2.966\n",
      "\n",
      "Interval 7 (60000 steps performed)\n",
      "10000/10000 [==============================] - 283s - reward: 0.0060   \n",
      "57 episodes - episode_reward: 1.053 [0.000, 4.000] - loss: 0.003 - mean_absolute_error: 0.013 - mean_q: 0.016 - mean_tau: 0.948 - ale.lives: 2.962\n",
      "\n",
      "Interval 8 (70000 steps performed)\n",
      "10000/10000 [==============================] - 283s - reward: 0.0051   \n",
      "60 episodes - episode_reward: 0.850 [0.000, 4.000] - loss: 0.003 - mean_absolute_error: 0.018 - mean_q: 0.021 - mean_tau: 0.940 - ale.lives: 2.914\n",
      "\n",
      "Interval 9 (80000 steps performed)\n",
      "10000/10000 [==============================] - 289s - reward: 0.0062   \n",
      "56 episodes - episode_reward: 1.089 [0.000, 4.000] - loss: 0.003 - mean_absolute_error: 0.022 - mean_q: 0.028 - mean_tau: 0.932 - ale.lives: 2.874\n",
      "\n",
      "Interval 10 (90000 steps performed)\n",
      "10000/10000 [==============================] - 289s - reward: 0.0063   \n",
      "56 episodes - episode_reward: 1.143 [0.000, 3.000] - loss: 0.003 - mean_absolute_error: 0.026 - mean_q: 0.033 - mean_tau: 0.924 - ale.lives: 2.937\n",
      "\n",
      "Interval 11 (100000 steps performed)\n",
      "10000/10000 [==============================] - 289s - reward: 0.0061   \n",
      "57 episodes - episode_reward: 1.053 [0.000, 4.000] - loss: 0.003 - mean_absolute_error: 0.030 - mean_q: 0.038 - mean_tau: 0.916 - ale.lives: 2.926\n",
      "\n",
      "Interval 12 (110000 steps performed)\n",
      "10000/10000 [==============================] - 289s - reward: 0.0055   \n",
      "57 episodes - episode_reward: 0.965 [0.000, 4.000] - loss: 0.003 - mean_absolute_error: 0.034 - mean_q: 0.043 - mean_tau: 0.908 - ale.lives: 2.998\n",
      "\n",
      "Interval 13 (120000 steps performed)\n",
      "10000/10000 [==============================] - 290s - reward: 0.0072   \n",
      "53 episodes - episode_reward: 1.377 [0.000, 5.000] - loss: 0.003 - mean_absolute_error: 0.039 - mean_q: 0.050 - mean_tau: 0.900 - ale.lives: 2.962\n",
      "\n",
      "Interval 14 (130000 steps performed)\n",
      " 8249/10000 [=======================>......] - ETA: 50s - reward: 0.0069done, took 2873.143 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1ea7bb18a58>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=500000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Saving the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('fire_incept_weights.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Loading the saved weights (of 0.5M steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('fire_incept_weights.h5f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Re-Training the model (for 2M steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "\n",
    "dqn = DQNAgent(model=model_fire_incept, nb_actions=nb_actions, policy=policy, memory=memory, processor=GameProcess(),\n",
    "               gamma=.99, target_model_update=10000, train_interval=4, delta_clip=1.)\n",
    "\n",
    "dqn.compile(Adam(lr=0.00025), metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.fit(env, nb_steps=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Saving final weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.save_weights('fire_incept_weights.h5f', overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dqn.load_weights('fire_incept_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.reset()\n",
    "dqn.test(env, nb_episodes=2, visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trained on: Intel® Xeon® Processor E5, 2.40 GHz, Nvidia Quadro K4200\n",
    "# Bhartendu Thakur, Machine Learning & Computing\n",
    "# https://in.mathworks.com/matlabcentral/profile/authors/10083740-bhartendu?&detail=fileexchange\n",
    "# https://in.linkedin.com/in/bhartendu-thakur-56bb6285"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
